# 【论文精读】多模态鲁棒性经典工作

> 主播从今天开始尽量坚持周更啦，也欢迎家人们多多监督催更！这次为大家带来的是多模态鲁棒性方向做的两篇工作，显著提升视觉语言模型在受图像攻击的情景下鲁棒性，并在Zero-shot数据样本上面展现出良好的性能，体现出它的可迁移性。这两篇工作也非常经典，主播也是学到了很多！

## 奠基之作：CLIP
CLIP这篇论文截止目前引用量已经达到了三万五千多，可见它的地位是如此之高。简而言之，它做了一个这样的事情：通过对比学习，**实现一组文本和图像的对齐**。虽然很简单，但是这篇文章发表于2021年，那时类ChatGPT的Decoder-only-LLM还没有发展壮大，所以这篇文章是一篇具有很高的前瞻性的好文章，奠定了最初的「图文结合」。

主播没有详细看这篇文章的实验部分和研究的动机，只是详细看了一下这篇论文提出的方法和架构：
![[Pasted image 20251028231709.png]]
「大道至简，简中藏巧，巧极方为精妙。」最精妙的架构就潜藏在最简单的结构中。这篇工作的结构非常简单：输入一组位置和顺序严格相对应的图文：
- 文字经过文字编码器进行编码，生成n个嵌入向量。
- 图片经过图片编码器以同样的方式，生成n个嵌入向量。
在此之后这些向量交叉相点积，得到一个矩阵$S_{ij}$, 其中i行j列代表的是图像$x_i$，文本$t_j$，那么它的损失函数是什么呢？很简单，就是使得图像$x_i$和描述图像$x_i$的文本$t_i$之间的嵌入向量「尽可能相近」，图像$x_i$和非描述图像$x_i$的文本$t_j$之间的嵌入向量「尽可能远离」，作者用一个简单的CrossEntropy来描述了这个损失函数：
$$L=-\frac{1}{2N}(\sum_{i=1}^nlog\frac{exp(S_{ii})}{\sum_{i=1}^Nexp(S_{ij})} + \sum_{i=1}^nlog\frac{exp(S_{ii})}{\sum_{j=1}^Nexp(S_{ij})})$$
一边是文本的，另一边是图的，这让模型能够在图和文本之间通过「对比学习」实现图文之间的对齐。

## TeCoA：鲁棒性的进阶之作
### 动机
虽然CLIP开启了「创世之作」，但是CLIP在鲁棒性方面显得极其脆弱，稍微受到一点攻击，识别率就会大大下降，鲁棒性及其不佳，那么TeCoA就是使用一种对抗的训练机制让模型的鲁棒性大大提升。

### 架构
TeCoA（Text-guided Contrastive Adversarial training）采用了一种「文本引导的对抗鲁棒性」策略，其核心思想是：**在训练过程中对图像模态进行对抗扰动生成，并利用文本模态提供的语义约束进行鲁棒对齐**。  
整体架构仍然基于预训练好的CLIP的图像编码器与文本编码器，但在训练阶段多了一条对抗样本生成路径与一致性约束路径：

1. **对抗扰动生成模块**  
   对输入图像$x$，通过快速梯度符号法（FGSM）或投影梯度下降（PGD）生成对抗样本$x_{adv}=x+\delta$，其中扰动$\delta$受限于$L_\infty$或$L_2$范数约束，主播没有详细了解这种方法，只是大概知道这样生成的扰动能够使得图像的「偏移」最大：  
   $$x_{adv} = x + \epsilon \cdot \text{sign}(\nabla_x L_{contrastive}(x, t))$$  
   这样可以使得模型在视觉模态上面对轻微的像素扰动时仍保持正确的语义判断。

2. **文本引导的鲁棒对齐模块**  
   模型在计算对比损失时，除了传统的正样本对$(x, t)$与负样本对$(x, t')$，还会引入对抗样本$(x_{adv}, t)$，使得模型不仅要对齐干净图像与文本，还要确保**对抗样本与文本的一致性**。  
   最终损失函数写作：
   $$
   L_{TeCoA} = L_{CLIP}(x, t) + \lambda \cdot L_{CLIP}(x_{adv}, t)
   $$
   其中$\lambda$用于平衡标准样本与对抗样本的训练权重。该设计可视为一种「鲁棒对比学习」。

3. **对抗特征一致性**  
   除了语义对齐，TeCoA还引入了一个特征层一致性约束：
   $$
   L_{feat} = \|f(x) - f(x_{adv})\|_2^2
   $$
   通过让干净图像与对抗图像在嵌入空间中保持接近，从而稳定视觉编码器的特征空间结构，这也就是「对抗」的核心。

### 实验结果
作者在ImageNet、CIFAR-10、CIFAR-100以及多模态基准如MS-COCO和Flickr30K上进行了验证。主要实验结论如下：
![[Pasted image 20251029141914.png]]

- **鲁棒性显著提升**：在FGSM、PGD攻击下，TeCoA的Top-1准确率相比CLIP提升了约20–30个百分点。  
- **零样本迁移性保持**：在未见过的下游任务（例如零样本分类与检索）中，TeCoA的性能仅略低于原CLIP，表明鲁棒增强并未显著破坏语义对齐。  
- **消融实验**表明文本引导的对抗路径（Text-guided Adversarial Branch）对鲁棒性贡献最大，而特征一致性损失主要提升模型在自然噪声下的稳健性。

---

## FARE：多模态鲁棒性的新范式
### 动机
TeCoA的提升主要体现在图像模态，但在多模态协同场景中，攻击往往可以同时作用于图像与文本。例如，通过改变文本描述或引入模糊语义，同样会破坏模型的判断。FARE（Feature-level Adversarial Robustness Enhancement）针对这一点，提出了**多模态特征层级的对抗鲁棒性提升机制**。

### 架构
FARE的整体框架仍以CLIP为骨干，但其关键创新点是引入了**双模态特征扰动**与**对齐平衡器（Alignment Balancer）**：

1. **双模态对抗扰动（Bi-modal Adversarial Perturbation）**  
   不仅对图像$x$生成扰动$x_{adv}$，同时对文本嵌入$e_t$添加扰动：
   $$
   e_{t,adv} = e_t + \eta \cdot \text{sign}(\nabla_{e_t} L_{contrastive}(x, t))
   $$
   这一步可以理解为「扰动图像与文本的语义空间」，模拟跨模态攻击。

2. **对齐平衡器（Alignment Balancer）**  
   由于双模态扰动可能破坏原有的语义对齐，FARE引入了一个动态平衡系数$\alpha$，用于自适应调节视觉与文本模态的对抗强度：
   $$
   L_{FARE} = L_{CLIP}(x, t) + \alpha L_{CLIP}(x_{adv}, t) + (1-\alpha)L_{CLIP}(x, t_{adv})
   $$
   其中$\alpha$通过模型训练动态调整，防止过度偏向某一模态。

3. **特征正则化与判别器一致性（Feature Regularization & Discriminator Consistency）**  
   为防止扰动后特征分布塌陷，FARE加入了特征分布约束：
   $$
   L_{reg} = \text{KL}(p(f(x)) \| p(f(x_{adv})))
   $$
   并利用轻量判别器来监督扰动样本的可分性，从而强化鲁棒性。

### 实验结果
作者在多个鲁棒性基准（包括ImageNet-A、ImageNet-R、CIFAR-10-C、CIFAR-100-C）上进行了测试。结果显示：
![[Pasted image 20251029142026.png]]

- **跨模态鲁棒性提升显著**：相比TeCoA，FARE在图文检索鲁棒性上平均提升12.6%，在文本扰动攻击（如词替换、语义反转）下仍保持较高精度。  
- **可迁移性增强**：FARE在下游的视觉问答（VQA）和图文匹配任务中，依然保持与原CLIP相当的性能。  
- **泛化性良好**：在未参与对抗训练的其他模态数据集上（如NoisyCOCO），模型仍能保持稳定表现。
虽然主播这边有一些疑问，似乎是RoBustCLIP仅仅在大语言模型的问题上面表现得比较好，但是主播个人的意见看来是专门针对大语言模型方向做的优化。

---

### 总结
那么这两篇鲁棒性的工作之间，有什么异同点呢？主播对比了一下这三种不同的模型之间的异同：

| 模型        | 对抗类型       | 鲁棒性提升点    | 零样本性能变化 |
| --------- | ---------- | --------- | ------- |
| **CLIP**  | 无          | 基准        | 基准      |
| **TeCoA** | 图像模态对抗     | 图像鲁棒性↑↑   | 基本保持    |
| **FARE**  | 图像+文本双模态对抗 | 全模态鲁棒性↑↑↑ | 稍有下降    |

TeCoA和FARE标志着多模态鲁棒性研究从「单模态鲁棒」迈向「跨模态协同鲁棒」，为后续视觉语言模型（如BLIP-2、Flamingo）的鲁棒性增强提供了重要思路。

好了，这期的论文精读就到这里啦，主播也会在下一期周更，同时也会不定期上传一些AI讯息解读和论文精读，欢迎各路大佬指路提建议！点个关注，实时更新AI讯息和论文！